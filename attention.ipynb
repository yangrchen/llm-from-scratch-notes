{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99bfc1a",
   "metadata": {},
   "source": [
    "# Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875067c6",
   "metadata": {},
   "source": [
    "Self-attention is a mechanism that allows each position in the input sequence to consider the relevancy of, or \"attend to,\" all other positions in the same sequence when computing the represntatino of a sequence. Self-attention is a key compinent of contemporary LLMs based on the transformer architecture, such as the GPT series.\n",
    "\n",
    "The goal of self-attention is to compute a context vector for each input element that combines information from all other input elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56ffb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15, 0.89],\n",
    "        [0.55, 0.87, 0.66],\n",
    "        [0.57, 0.85, 0.64],\n",
    "        [0.22, 0.58, 0.33],\n",
    "        [0.77, 0.25, 0.10],\n",
    "        [0.05, 0.80, 0.55],\n",
    "    ]\n",
    ")\n",
    "\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df45725",
   "metadata": {},
   "source": [
    "Beyond viewing the dot product operation as a mathematical toon that combines two vectors to yield a scalar value, the dot product is a measure of similarity because it quantifies how closely two vectors are aligned: a higher dot product indicates a greater degree of alignment or similarity between the vectors. In the context of self-attention mechanisms, the dot product determines the extent to which each element in a sequence focuses on, or \"attends to,\" any other element: the higher the dot product, the higher the similarity and attention score between two elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb1d426",
   "metadata": {},
   "source": [
    "## Normalizing the attention scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248aee6",
   "metadata": {},
   "source": [
    "Below is a simple normalization to obtain attention weights that sum to 1. In practice, it's more common to use the softmax function for normalization. This approach is better at managing extreme values and offers more favorable gradient properties during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717013fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ed12c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49ce2b",
   "metadata": {},
   "source": [
    "Context vector $z_2$ is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b83129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39153d",
   "metadata": {},
   "source": [
    "## Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82948752",
   "metadata": {},
   "source": [
    "Using Python for-loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2342d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1018cd",
   "metadata": {},
   "source": [
    "Using matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b2413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905fcff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "attn_weights.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8196e59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c043a",
   "metadata": {},
   "source": [
    "# Self-attention through scaled dot-product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00684059",
   "metadata": {},
   "source": [
    "## Step-by-step implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a62a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc42d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60113569",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4288926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cd70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f609d",
   "metadata": {},
   "source": [
    "### Scaling and normalizing using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07fd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ed0c5",
   "metadata": {},
   "source": [
    "The reason for the normalization by the embedding simension size is to improve the training performance by avoiding small gradients. For instance, when scaling up the embedding dimension, which is typically greater than 1,000 for GPT-like LLMs, large dot products can result in very small gradients during backpropagation due to the softmax function applied to them. As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. These small gradients can drastically slow down learning or cause training to stagnate.\n",
    "\n",
    "Similar to when we computed the context vector as a weighted sum over the input vectors, we now compute the context vector as a weighted sum over the value vectors. Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb3b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 2]), torch.Size([6]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "values.shape, attn_weights_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a03055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080604df",
   "metadata": {},
   "source": [
    "## General attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904875cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Tensors of shape (batch_size, seq_len, d_out)\n",
    "        query = x @ self.W_query\n",
    "        key = x @ self.W_key\n",
    "        value = x @ self.W_value\n",
    "\n",
    "        d_k = key.shape[-1]\n",
    "\n",
    "        attn_scores = query @ key.transpose(-1, -2)\n",
    "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
    "        return attn_weights @ value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f7386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2996, 0.8053],\n",
       "        [0.3061, 0.8210],\n",
       "        [0.3058, 0.8203],\n",
       "        [0.2948, 0.7939],\n",
       "        [0.2927, 0.7891],\n",
       "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "self_attention = SelfAttentionV1(3, 2)\n",
    "self_attention(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbbba81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 4],\n",
       "         [2, 5],\n",
       "         [3, 6]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.tensor([[[1, 2, 3], [4, 5, 6]]]).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        query = self.W_query(x)\n",
    "        key = self.W_key(x)\n",
    "        value = self.W_value(x)\n",
    "\n",
    "        attn_scores = query @ key.transpose(-1, -2)\n",
    "        attn_weights = torch.softmax(attn_scores / key.shape[-1] ** 0.5, dim=-1)\n",
    "\n",
    "        return attn_weights @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831c877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0739,  0.0713],\n",
       "        [-0.0748,  0.0703],\n",
       "        [-0.0749,  0.0702],\n",
       "        [-0.0760,  0.0685],\n",
       "        [-0.0763,  0.0679],\n",
       "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "self_attention_v2 = SelfAttentionV2(3, 2)\n",
    "self_attention_v2(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0834241",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_params = list(self_attention.parameters())\n",
    "s2_params = list(self_attention_v2.parameters())\n",
    "\n",
    "for idx, param in enumerate(s1_params):\n",
    "    param.data = s2_params[idx].data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b024c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0739,  0.0713],\n",
       "        [-0.0748,  0.0703],\n",
       "        [-0.0749,  0.0702],\n",
       "        [-0.0760,  0.0685],\n",
       "        [-0.0763,  0.0679],\n",
       "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "self_attention(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e10f1",
   "metadata": {},
   "source": [
    "## Causal attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5337cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = self_attention_v2.W_query(inputs)\n",
    "keys = self_attention_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ef318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
       "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "display(mask_simple)\n",
    "\n",
    "masked_simple = attn_weights * mask_simple\n",
    "display(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355be27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
       "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "masked_simple_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac40731",
   "metadata": {},
   "source": [
    "When we apply a mask and then renormalize the attention weights, it might initially appear that information from future tokens (which we intend to mask) could still influence the current token because their values are part of the softmax calculation. However, the key insight is that when we renormalize the attention weights after masking, what we're essentially doing is recalculating the softmax over a smaller subset (since masked positions don't contribute to the softmax value).\n",
    "\n",
    "The mathematical elegance of softmax is that despite initlaiiy including all positions in the denominator, after masking and renormalizing, the effect of the masked positions is nullified--they don't contribute to the softmax score in any meaningful way.\n",
    "\n",
    "In simpler terms, after masking and renormalization, the distribution of attention weights is as if it was calculated only among the unmasked positions to begin with. This ensures there's no information leakage from future (or otherwise masked) tokens as we intended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31396d",
   "metadata": {},
   "source": [
    "## More efficient causal attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0cb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
       "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
       "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
       "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44a622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
       "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
       "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7e661",
   "metadata": {},
   "source": [
    "### Masking w/ dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528badb7",
   "metadata": {},
   "source": [
    "**Dropout** is a technique where randomly selected hidden layer units are ignored during training, effectively \"dropping\" them out. This method helps prevent overfitting by ensuring that a model does no become overly reliant on any specific set of hidden layer units. Dropout is only used during training and is disabled afterward.\n",
    "\n",
    "In the transformer architecture, dropout in the attention mechanism is typically applied at two specific times: after calculating the attention weights or after applying the attention weights to the value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d160d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 0., 2., 2., 0.],\n",
       "        [0., 0., 0., 2., 0., 2.],\n",
       "        [2., 2., 2., 2., 0., 2.],\n",
       "        [0., 2., 2., 0., 0., 2.],\n",
       "        [0., 2., 0., 2., 0., 2.],\n",
       "        [0., 2., 2., 2., 2., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "display(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c2cd7f",
   "metadata": {},
   "source": [
    "When applying dropout to the weight matrix at a rate of 0.5, half of the elements are randomly set to 0. To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a factor of $\\frac{1}{0.5} = 2$. This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains consistent during both training and inference phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6849bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
       "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "display(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b205157",
   "metadata": {},
   "source": [
    "# Implementing causal attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494c290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = torch.stack([inputs, inputs], dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce193963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Tensor shapes: (bs, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Matrix multiplication creates tensor shape (bs, num_tokens, num_tokens)\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "\n",
    "        # Apply mask truncated to num_tokens\n",
    "        masked = attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "        )\n",
    "        attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=-1)\n",
    "\n",
    "        return attn_weights @ values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad407d6",
   "metadata": {},
   "source": [
    "Using `register_buffer()` allows the buffer to be automatically moved to the appropriate device along with our model. We don't need to manually ensure these tensors are on the same device as the model parameters, avoiding device mismatch errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27de40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "causal_attn = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = causal_attn(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59154fa",
   "metadata": {},
   "source": [
    "# Multi-headed Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb9827",
   "metadata": {},
   "source": [
    "The term **\"multi-head\"** refers to dividing the attention mechanism into multiple \"heads\", each operating independently. In this context, a single causal attention module can be considered a single-head attention.\n",
    "\n",
    "In practical terms, multi-head attention involves creating multiple instances of the self-attention mechanism and then stacking their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d10164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d6dc1",
   "metadata": {},
   "source": [
    "Main idea behind multi-head attention is to run the attention mechanism multiple times with different, learned linear projections. This implementation processes heads in a sequential manner. To process in parallel we compute the outputs for all attention heads simultaneously via matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c224cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
       "\n",
       "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
       "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
       "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
       "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
       "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
       "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "display(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e607cc4",
   "metadata": {},
   "source": [
    "## Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52447e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionCopy(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, seq_len, d_in = x.shape\n",
    "\n",
    "        # Tensor shapes: (bs, seq_len, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Split the weight matrices into num_heads\n",
    "        queries = queries.view(bs, seq_len, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(bs, seq_len, self.num_heads, self.head_dim)\n",
    "        values = values.view(bs, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose from shape (bs, seq_len, num_heads, head_dim) to (bs, num_heads, seq_len, head_dim)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute the attention scores for each head. (bs, num_heads, seq_len, seq_len)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        # Mask truncated to seq_len\n",
    "        mask = self.mask.bool()[:seq_len, :seq_len]\n",
    "\n",
    "        # Mask applied to attention scores\n",
    "        attn_scores.masked_fill_(mask, -torch.inf)\n",
    "\n",
    "        # Calculate the attention weights w/ dropout\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Tensor shape: (bs, seq_len, num_heads, head_dim)\n",
    "        context_vecs = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combines heads, where d_out = num_heads * head_dim\n",
    "        context_vecs = context_vecs.contiguous().view(bs, seq_len, self.d_out)\n",
    "\n",
    "        # Adds an optional linear projection\n",
    "        context_vecs = self.out_proj(context_vecs)\n",
    "\n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a14db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        # Reduces the projection dim to match the desired output dim\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        # Uses a linear layer to combine head outputs\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Tensor shapes: (bs, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Implicitly split the matrices by adding a num_heads dimension\n",
    "        queries = queries.view(bs, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(bs, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(bs, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transposes from shape (bs, num_tokens, num_heads, head_dim) to (bs, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Conputes the dot product for each head\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        # Masks truncated to num_tokens\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Apply mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Tensor shape: (bs, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combines heads, where d_out = num_heads * head_dim\n",
    "        context_vec = context_vec.contiguous().view(bs, num_tokens, self.d_out)\n",
    "\n",
    "        # Adds an optional linear projection\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8a624",
   "metadata": {},
   "source": [
    "![Multi-headed Attention](mha.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9bbcfc",
   "metadata": {},
   "source": [
    "Previously, `MultiHeadAttentionWrapper` stacked multiple single-head attention layers that were combined into a multi-head attention layer which resulted in multiple matrix multiplications. The `MultiHeadAttention` class starts with a multi-head layer and splits this layer into individual attention heads based on `num_heads`, `head_dim`.\n",
    "\n",
    " - Splitting of query, key, and value tensors is achieved thorugh tensor reshaping and transposing operations using `.view` and `.transpose`. Input is first projected through linear layers and then reshaped to represent multiple heads.\n",
    " - Key operation is to split the `d_out` dimension into `num_heads` and `head_dim`. The splitting into multiple heads occurs through `.view` when the projected tensors are reshaped into `(bs, seq_len, num_heads, head_dim)`\n",
    " - Tensors are then transposed to bring the `num_heads` dimension before the `seq_len` dimension which results in shape `(bs, num_heads, seq_len, head_dim)`. This alighs the queries, keys, and values across the different heads and allows for batched matrix multiplications. \n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
